/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks
package public
package only_pk_columns

import cats.instances.list.catsStdInstancesForList
import doobie.free.connection.ConnectionIO
import doobie.postgres.syntax.FragmentOps
import doobie.syntax.SqlInterpolator.SingleFragment.fromWrite
import doobie.syntax.string.toSqlInterpolator
import doobie.util.Write
import doobie.util.meta.Meta
import doobie.util.update.Update
import fs2.Stream
import typo.dsl.DeleteBuilder
import typo.dsl.SelectBuilder
import typo.dsl.SelectBuilderSql
import typo.dsl.UpdateBuilder

class OnlyPkColumnsRepoImpl extends OnlyPkColumnsRepo {
  override def delete: DeleteBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    DeleteBuilder(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure)
  }
  override def deleteById(compositeId: OnlyPkColumnsId): ConnectionIO[Boolean] = {
    sql"""delete from "public"."only_pk_columns" where "key_column_1" = ${fromWrite(compositeId.keyColumn1)(new Write.Single(Meta.StringMeta.put))} AND "key_column_2" = ${fromWrite(compositeId.keyColumn2)(new Write.Single(Meta.IntMeta.put))}""".update.run.map(_ > 0)
  }
  override def deleteByIds(compositeIds: Array[OnlyPkColumnsId]): ConnectionIO[Int] = {
    val keyColumn1 = compositeIds.map(_.keyColumn1)
    val keyColumn2 = compositeIds.map(_.keyColumn2)
    sql"""delete
          from "public"."only_pk_columns"
          where ("key_column_1", "key_column_2")
          in (select unnest(${keyColumn1}), unnest(${keyColumn2}))
       """.update.run
    
  }
  override def insert(unsaved: OnlyPkColumnsRow): ConnectionIO[OnlyPkColumnsRow] = {
    sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          values (${fromWrite(unsaved.keyColumn1)(new Write.Single(Meta.StringMeta.put))}, ${fromWrite(unsaved.keyColumn2)(new Write.Single(Meta.IntMeta.put))}::int4)
          returning "key_column_1", "key_column_2"
       """.query(using OnlyPkColumnsRow.read).unique
  }
  override def insertStreaming(unsaved: Stream[ConnectionIO, OnlyPkColumnsRow], batchSize: Int = 10000): ConnectionIO[Long] = {
    new FragmentOps(sql"""COPY "public"."only_pk_columns"("key_column_1", "key_column_2") FROM STDIN""").copyIn(unsaved, batchSize)(using OnlyPkColumnsRow.text)
  }
  override def select: SelectBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    SelectBuilderSql(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.read)
  }
  override def selectAll: Stream[ConnectionIO, OnlyPkColumnsRow] = {
    sql"""select "key_column_1", "key_column_2" from "public"."only_pk_columns"""".query(using OnlyPkColumnsRow.read).stream
  }
  override def selectById(compositeId: OnlyPkColumnsId): ConnectionIO[Option[OnlyPkColumnsRow]] = {
    sql"""select "key_column_1", "key_column_2" from "public"."only_pk_columns" where "key_column_1" = ${fromWrite(compositeId.keyColumn1)(new Write.Single(Meta.StringMeta.put))} AND "key_column_2" = ${fromWrite(compositeId.keyColumn2)(new Write.Single(Meta.IntMeta.put))}""".query(using OnlyPkColumnsRow.read).option
  }
  override def selectByIds(compositeIds: Array[OnlyPkColumnsId]): Stream[ConnectionIO, OnlyPkColumnsRow] = {
    val keyColumn1 = compositeIds.map(_.keyColumn1)
    val keyColumn2 = compositeIds.map(_.keyColumn2)
    sql"""select "key_column_1", "key_column_2"
          from "public"."only_pk_columns"
          where ("key_column_1", "key_column_2")
          in (select unnest(${keyColumn1}), unnest(${keyColumn2}))
       """.query(using OnlyPkColumnsRow.read).stream
    
  }
  override def selectByIdsTracked(compositeIds: Array[OnlyPkColumnsId]): ConnectionIO[Map[OnlyPkColumnsId, OnlyPkColumnsRow]] = {
    selectByIds(compositeIds).compile.toList.map { rows =>
      val byId = rows.view.map(x => (x.compositeId, x)).toMap
      compositeIds.view.flatMap(id => byId.get(id).map(x => (id, x))).toMap
    }
  }
  override def update: UpdateBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    UpdateBuilder(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.read)
  }
  override def upsert(unsaved: OnlyPkColumnsRow): ConnectionIO[OnlyPkColumnsRow] = {
    sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          values (
            ${fromWrite(unsaved.keyColumn1)(new Write.Single(Meta.StringMeta.put))},
            ${fromWrite(unsaved.keyColumn2)(new Write.Single(Meta.IntMeta.put))}::int4
          )
          on conflict ("key_column_1", "key_column_2")
          do update set "key_column_1" = EXCLUDED."key_column_1"
          returning "key_column_1", "key_column_2"
       """.query(using OnlyPkColumnsRow.read).unique
  }
  override def upsertBatch(unsaved: List[OnlyPkColumnsRow]): Stream[ConnectionIO, OnlyPkColumnsRow] = {
    Update[OnlyPkColumnsRow](
      s"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          values (?,?::int4)
          on conflict ("key_column_1", "key_column_2")
          do nothing
          returning "key_column_1", "key_column_2""""
    )(using OnlyPkColumnsRow.write)
    .updateManyWithGeneratedKeys[OnlyPkColumnsRow]("key_column_1", "key_column_2")(unsaved)(using catsStdInstancesForList, OnlyPkColumnsRow.read)
  }
  /* NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  override def upsertStreaming(unsaved: Stream[ConnectionIO, OnlyPkColumnsRow], batchSize: Int = 10000): ConnectionIO[Int] = {
    for {
      _ <- sql"""create temporary table only_pk_columns_TEMP (like "public"."only_pk_columns") on commit drop""".update.run
      _ <- new FragmentOps(sql"""copy only_pk_columns_TEMP("key_column_1", "key_column_2") from stdin""").copyIn(unsaved, batchSize)(using OnlyPkColumnsRow.text)
      res <- sql"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
                   select * from only_pk_columns_TEMP
                   on conflict ("key_column_1", "key_column_2")
                   do nothing
                   ;
                   drop table only_pk_columns_TEMP;""".update.run
    } yield res
  }
}
