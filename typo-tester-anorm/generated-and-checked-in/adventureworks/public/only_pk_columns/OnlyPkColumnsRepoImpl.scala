/**
 * File has been automatically generated by `typo`.
 *
 * IF YOU CHANGE THIS FILE YOUR CHANGES WILL BE OVERWRITTEN.
 */
package adventureworks
package public
package only_pk_columns

import anorm.BatchSql
import anorm.NamedParameter
import anorm.ParameterValue
import anorm.SqlStringInterpolation
import anorm.ToStatement
import java.sql.Connection
import scala.annotation.nowarn
import typo.dsl.DeleteBuilder
import typo.dsl.SelectBuilder
import typo.dsl.SelectBuilderSql
import typo.dsl.UpdateBuilder

class OnlyPkColumnsRepoImpl extends OnlyPkColumnsRepo {
  override def delete: DeleteBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    DeleteBuilder(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure)
  }
  override def deleteById(compositeId: OnlyPkColumnsId)(implicit c: Connection): Boolean = {
    SQL"""delete from "public"."only_pk_columns" where "key_column_1" = ${ParameterValue(compositeId.keyColumn1, null, ToStatement.stringToStatement)} AND "key_column_2" = ${ParameterValue(compositeId.keyColumn2, null, ToStatement.intToStatement)}""".executeUpdate() > 0
  }
  override def deleteByIds(compositeIds: Array[OnlyPkColumnsId])(implicit c: Connection): Int = {
    val keyColumn1 = compositeIds.map(_.keyColumn1)
    val keyColumn2 = compositeIds.map(_.keyColumn2)
    SQL"""delete
          from "public"."only_pk_columns"
          where ("key_column_1", "key_column_2")
          in (select unnest(${keyColumn1}), unnest(${keyColumn2}))
       """.executeUpdate()
    
  }
  override def insert(unsaved: OnlyPkColumnsRow)(implicit c: Connection): OnlyPkColumnsRow = {
    SQL"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          values (${ParameterValue(unsaved.keyColumn1, null, ToStatement.stringToStatement)}, ${ParameterValue(unsaved.keyColumn2, null, ToStatement.intToStatement)}::int4)
          returning "key_column_1", "key_column_2"
       """
      .executeInsert(OnlyPkColumnsRow.rowParser(1).single)
    
  }
  override def insertStreaming(unsaved: Iterator[OnlyPkColumnsRow], batchSize: Int = 10000)(implicit c: Connection): Long = {
    streamingInsert(s"""COPY "public"."only_pk_columns"("key_column_1", "key_column_2") FROM STDIN""", batchSize, unsaved)(OnlyPkColumnsRow.text, c)
  }
  override def select: SelectBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    SelectBuilderSql(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.rowParser)
  }
  override def selectAll(implicit c: Connection): List[OnlyPkColumnsRow] = {
    SQL"""select "key_column_1", "key_column_2"
          from "public"."only_pk_columns"
       """.as(OnlyPkColumnsRow.rowParser(1).*)
  }
  override def selectById(compositeId: OnlyPkColumnsId)(implicit c: Connection): Option[OnlyPkColumnsRow] = {
    SQL"""select "key_column_1", "key_column_2"
          from "public"."only_pk_columns"
          where "key_column_1" = ${ParameterValue(compositeId.keyColumn1, null, ToStatement.stringToStatement)} AND "key_column_2" = ${ParameterValue(compositeId.keyColumn2, null, ToStatement.intToStatement)}
       """.as(OnlyPkColumnsRow.rowParser(1).singleOpt)
  }
  override def selectByIds(compositeIds: Array[OnlyPkColumnsId])(implicit c: Connection): List[OnlyPkColumnsRow] = {
    val keyColumn1 = compositeIds.map(_.keyColumn1)
    val keyColumn2 = compositeIds.map(_.keyColumn2)
    SQL"""select "key_column_1", "key_column_2"
          from "public"."only_pk_columns"
          where ("key_column_1", "key_column_2")
          in (select unnest(${keyColumn1}), unnest(${keyColumn2}))
       """.as(OnlyPkColumnsRow.rowParser(1).*)
    
  }
  override def selectByIdsTracked(compositeIds: Array[OnlyPkColumnsId])(implicit c: Connection): Map[OnlyPkColumnsId, OnlyPkColumnsRow] = {
    val byId = selectByIds(compositeIds).view.map(x => (x.compositeId, x)).toMap
    compositeIds.view.flatMap(id => byId.get(id).map(x => (id, x))).toMap
  }
  override def update: UpdateBuilder[OnlyPkColumnsFields, OnlyPkColumnsRow] = {
    UpdateBuilder(""""public"."only_pk_columns"""", OnlyPkColumnsFields.structure, OnlyPkColumnsRow.rowParser)
  }
  override def upsert(unsaved: OnlyPkColumnsRow)(implicit c: Connection): OnlyPkColumnsRow = {
    SQL"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          values (
            ${ParameterValue(unsaved.keyColumn1, null, ToStatement.stringToStatement)},
            ${ParameterValue(unsaved.keyColumn2, null, ToStatement.intToStatement)}::int4
          )
          on conflict ("key_column_1", "key_column_2")
          do update set "key_column_1" = EXCLUDED."key_column_1"
          returning "key_column_1", "key_column_2"
       """
      .executeInsert(OnlyPkColumnsRow.rowParser(1).single)
    
  }
  override def upsertBatch(unsaved: Iterable[OnlyPkColumnsRow])(implicit c: Connection): List[OnlyPkColumnsRow] = {
    def toNamedParameter(row: OnlyPkColumnsRow): List[NamedParameter] = List(
      NamedParameter("key_column_1", ParameterValue(row.keyColumn1, null, ToStatement.stringToStatement)),
      NamedParameter("key_column_2", ParameterValue(row.keyColumn2, null, ToStatement.intToStatement))
    )
    unsaved.toList match {
      case Nil => Nil
      case head :: rest =>
        new anorm.adventureworks.ExecuteReturningSyntax.Ops(
          BatchSql(
            s"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
                values ({key_column_1}, {key_column_2}::int4)
                on conflict ("key_column_1", "key_column_2")
                do nothing
                returning "key_column_1", "key_column_2"
             """,
            toNamedParameter(head),
            rest.map(toNamedParameter)*
          )
        ).executeReturning(OnlyPkColumnsRow.rowParser(1).*)
    }
  }
  /* NOTE: this functionality is not safe if you use auto-commit mode! it runs 3 SQL statements */
  override def upsertStreaming(unsaved: Iterator[OnlyPkColumnsRow], batchSize: Int = 10000)(implicit c: Connection): Int = {
    SQL"""create temporary table only_pk_columns_TEMP (like "public"."only_pk_columns") on commit drop""".execute(): @nowarn
    streamingInsert(s"""copy only_pk_columns_TEMP("key_column_1", "key_column_2") from stdin""", batchSize, unsaved)(OnlyPkColumnsRow.text, c): @nowarn
    SQL"""insert into "public"."only_pk_columns"("key_column_1", "key_column_2")
          select * from only_pk_columns_TEMP
          on conflict ("key_column_1", "key_column_2")
          do nothing
          ;
          drop table only_pk_columns_TEMP;""".executeUpdate()
  }
}
